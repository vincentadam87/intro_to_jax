{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bba0959",
   "metadata": {},
   "source": [
    "# Mnist Classification with Convolutional Neural Networks - with JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a896cab",
   "metadata": {},
   "source": [
    "We start with a few imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, random\n",
    "rng = random.PRNGKey(0)\n",
    "\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.example_libraries import stax\n",
    "from jax.example_libraries.stax import Dense, Relu, LogSoftmax, Conv, Flatten, Identity\n",
    "\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !git clone https://github.com/vincentadam87/intro_to_jax.git\n",
    "    import sys  \n",
    "    sys.path.insert(0,'/content/intro_to_jax/notebooks')\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb4175",
   "metadata": {},
   "source": [
    "Setting up a Convolutional Neural Network (CNN) model with stax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e216c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating \n",
    "# * a function to generate random inital weights and biases for a neural network\n",
    "# * the forward model (the predict function) of the neural network from image x to class label probability\n",
    "\n",
    "init_random_params, predict = stax.serial(\n",
    "    Conv(16,(3,3), padding=\"SAME\"),\n",
    "    Relu,\n",
    "    Conv(16, (3,3), padding=\"SAME\"),\n",
    "    Relu,\n",
    "    Flatten,\n",
    "    Dense(10),\n",
    "    LogSoftmax\n",
    ")\n",
    "\n",
    "# Note: the parameters will be stored as list of lists (of weight + bias)\n",
    "# Note: you can also change the architecture!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff348df",
   "metadata": {},
   "source": [
    "As before we need to declare the loss, accuracy and update functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74da15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss of the classifier: it is the log likelihood of the observations\n",
    "\n",
    "def loss(params, batch):\n",
    "    \"\"\"\n",
    "    :param params: the parameters of the neural network\n",
    "    :param batch: a list of the imput images and associated label\n",
    "    :return: a scalar number, the loss to be minimized\n",
    "    \"\"\"\n",
    "    # your code here (same as previous notebook)\n",
    "    \n",
    "@jit\n",
    "def accuracy(params, batch):\n",
    "    \"\"\"\n",
    "    The accuracy is the fraction of correct predictions.\n",
    "    :param params: the parameters of the neural network\n",
    "    :param batch: the imput images \n",
    "    :return: a scalar number, the accuracy\n",
    "    \"\"\"\n",
    "    # your code here (same as previous notebook)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the update method to run the training loop\n",
    "\n",
    "# Here we don't need to manually code the update. This will be done automatically later!\n",
    "# you can use the method `opt_update`\n",
    "\n",
    "@jit\n",
    "def update(i, opt_state, batch):\n",
    "    # your code here (same as previous notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4dbec1",
   "metadata": {},
   "source": [
    "Now the main learning routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018cfefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up training parameters\n",
    "step_size = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa329915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data and preprocessing (not that here we don't flatten the images)\n",
    "\n",
    "train_images, train_labels, test_images, test_labels = datasets.mnist(flatten_images=False)\n",
    "num_train = train_images.shape[0]\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f50930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data stream to easily get the batches\n",
    "def data_stream():\n",
    "    rng = npr.RandomState(0)\n",
    "    while True:\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield train_images[batch_idx], train_labels[batch_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63837a23",
   "metadata": {},
   "source": [
    "Now the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c96f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we initiate a SGD optimizer which provides the update function\n",
    "opt_init, opt_update, get_params = optimizers.sgd(step_size)\n",
    "\n",
    "\n",
    "batches = data_stream()\n",
    "\n",
    "_, init_params = init_random_params(rng, (-1, 28, 28, 1))\n",
    "opt_state = opt_init(init_params)\n",
    "\n",
    "# this is simply a counter\n",
    "itercount = itertools.count()\n",
    "\n",
    "# now iterate over epoch\n",
    "# for each epoch, iterate over batches\n",
    "# and update the weights\n",
    "# at the end of the epoch print the training and test accuracies\n",
    "\n",
    "    \n",
    "# your code here ...\n",
    "\n",
    "\n",
    "# Note: this will be slow unless you use a GPU. \n",
    "# If you use colab, you should change the runtime settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b5ff5",
   "metadata": {},
   "source": [
    "# Interpreting the convolutional weights\n",
    "\n",
    "Convolutional neural networks were inspired by receptive fields in visual area V1 of the brain.\n",
    "\n",
    "We know that the visual system consists of a sequence of feature extractors,\n",
    "starting from simple line, edge detectors.\n",
    "\n",
    "Convolutional neural networks learn similar strategies from data!\n",
    "Let's look at the learned feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights of the first layer (the one directly applied to the input image)\n",
    "# hint:  weights for the first layer are in params[0][0]\n",
    "\n",
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4359523",
   "metadata": {},
   "source": [
    "### Questions\n",
    "* Can you see the edge detector or other interpretable features in the weigths?\n",
    "* Play with different architectures, can you get a better accuracy?\n",
    "* can you plot the outcome of the first convolutional layer?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
