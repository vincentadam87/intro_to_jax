{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d8bdea",
   "metadata": {},
   "source": [
    "# Logistic regression with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff822cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "from jax import vmap\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7580595",
   "metadata": {},
   "source": [
    "# Learning as gradient-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a347ab",
   "metadata": {},
   "source": [
    "##  Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bbb47",
   "metadata": {},
   "source": [
    "In logistic regression, the assumption is that data is generated as follows \n",
    "\\begin{align}\n",
    "\\phi(x) &= wx + b\\\\\n",
    "y &\\sim Bernoulli( \\sigma( \\phi(x) ))\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1+e^{-x}}$ is the sigmoid function.\n",
    "\n",
    "a sample from $Bernoulli(p)$ is a biased coin flip with probability of Heads (=1) equal to $p$\n",
    "\n",
    "You have \n",
    "* data: $\\{X, Y\\}$ \n",
    "* a linear predictor : $\\rho(x) = w x + b$ (parameters are weights $w$ and bias $b$)\n",
    "* leading to predicted classes $p(y=1|x) = \\sigma(\\rho(x))$\n",
    "* a loss (log likelihood): $ l(w) = \\sum_n y_n \\,\\log\\,p(x) + (1-y_n)\\log(1 - p(x))$\n",
    "\n",
    "We could compute the gradient manually, but with jax we don't need to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be a tiny bit more sophisticated, we'll use a class to hold the parameters\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Params(NamedTuple):\n",
    "    \"\"\" \n",
    "    Class that acts as a container for the parameters.     \n",
    "    you can use it like param = Params(weights, bias)\n",
    "    \"\"\"\n",
    "    weight: jnp.ndarray\n",
    "    bias: jnp.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an initialization class for the weights\n",
    "\n",
    "def init(rng) -> Params:\n",
    "    \"\"\"Returns the initial model params.\n",
    "    :param rng: random number generator\n",
    "    :return: an instance of the 'Params' class\n",
    "    \"\"\"\n",
    "    weights_key, bias_key = jax.random.split(rng)\n",
    "    random_weight = jax.random.normal(weights_key, ()) * 3\n",
    "    random_bias = jax.random.normal(bias_key, ()) * 3\n",
    "    return Params(random_weight, random_bias)\n",
    "\n",
    "\n",
    "# we need the sigmoid for the pointwise activations\n",
    "def sigmoid(x: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    The sigmoid function x->1/(x + e^{-x})\n",
    "    \"\"\"\n",
    "    # your code here ...\n",
    "\n",
    "\n",
    "# we compute our prediction of the label being one\n",
    "def predictions(params: Params, x: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    The prediction p(y=1|x).\n",
    "    :param params: an instance of the class Params\n",
    "    :param x: the scalar input\n",
    "    :return: a probabilty p, 0<p<1 \n",
    "    \"\"\"\n",
    "    # your code here ...\n",
    "    \n",
    "    \n",
    "# again we define the loss (as defined above)\n",
    "def loss(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the least squares error of the model's predictions on x against y.\n",
    "    :param params: an instance of the class Params\n",
    "    :param x: the scalar input\n",
    "    :param y: the binary observation\n",
    "    :return: the scalar loss \n",
    "    \"\"\"\n",
    "    # your code here ...\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "# there is a convenient way to propagate the gradient through to the parameters\n",
    "# even for the more complex structure we now have.\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> Params:\n",
    "    \"\"\"\n",
    "    Performs one SGD update step on params using the given data.\n",
    "    \n",
    "    θ = θ - η grad l(θ)  (with η the learning rate) \n",
    "    \n",
    "    :param params: an instance of the class Params\n",
    "    :param x: the scalar input\n",
    "    :param y: the binary observation\n",
    "    :return: an instance of the Params class containing the updated parameters \n",
    "    \"\"\"\n",
    "    # code up the gradient and the update\n",
    "    # hint: with the Params class, jax.grad will produce a gradient with the same structure\n",
    "    # you can use the function jax.tree_map to apply the gradient to the parameters.\n",
    "    \n",
    "    # your code here ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95a997",
   "metadata": {},
   "source": [
    "Let's generate some data!\n",
    "\n",
    "\\begin{align}\n",
    "x &\\sim {\\cal N}(0,1)\\quad \\text{this means x is Gaussian with mean 0 and variance 1}\\\\\n",
    "\\phi(x) &= wx + b\\\\\n",
    "y &\\sim Bernoulli( \\sigma( \\phi(x) ))\\\\\n",
    "\\end{align}\n",
    "\n",
    "for some chosen values for $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4673c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to prepare jax to sample random variable\n",
    "# see https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/05-random-numbers.ipynb\n",
    "# for more details\n",
    "rng = jax.random.PRNGKey(42) \n",
    "x_rng, noise_rng = jax.random.split(rng)\n",
    "\n",
    "# Generate true data from p = w*x + b, y ~ Bernoulli(p)\n",
    "\n",
    "# we choose some ground true parameters w=2, b=-1 \n",
    "# create an instance of the Params class for these ground true parameters\n",
    "# your code here ...\n",
    "\n",
    "\n",
    "\n",
    "# we generate 500 data points\n",
    "num_data = 500 \n",
    "# your code here ...\n",
    "# hint: for x,  you can use `jax.random.normal`\n",
    "# hint: for y,  you can use `jax.random.bernoulli`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1d598",
   "metadata": {},
   "source": [
    "Let's plot the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802adb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot y as a function of x\n",
    "# plot the predictions for the true parameters p(x) = \\sigma( \\phi(x) )\n",
    "\n",
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c49f03",
   "metadata": {},
   "source": [
    "Now let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the update function for 200 iterations\n",
    "# and store the intermediate values of the parameters and loss\n",
    "\n",
    "# initializing some arrays for storage\n",
    "num_iterations = 400\n",
    "weights = np.zeros((num_iterations,))\n",
    "biases = np.zeros((num_iterations,))\n",
    "losses = np.zeros((num_iterations,))\n",
    "\n",
    "\n",
    "params = init(rng)\n",
    "for it in range(num_iterations):\n",
    "    # your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019f537",
   "metadata": {},
   "source": [
    "Let's plot the learning curve and evolution of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75242c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c46f9",
   "metadata": {},
   "source": [
    "## What if there is really a lot of data?\n",
    "\n",
    "\n",
    "If there is a lot of data (think billions), there is no way to evaluate the loss\n",
    "on the whole dataset as we have done so far.\n",
    "\n",
    "We can evaluate the loss on a randomly sampled subset of the data, \n",
    "which leads to an unbiased estimator of the total loss.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{l}(w) &=  l(x_{batch}, y_{batch}, \\theta)\\\\\n",
    "\\mathbb{E}[\\hat{l}(w)] &= l(w)\n",
    "\\end{align}\n",
    "\n",
    "If we replace the gradient of the loss in gradient descent by the gradient of \n",
    "an estimator of the loss we get stochastica gradient descent (SGD)\n",
    "\n",
    "\n",
    "\n",
    "This corresponds to iterate gradient descent steps on batch approximation of the loss.\n",
    "\\begin{align}\n",
    "\\theta_{t+1} &= \\theta_{t} - \\eta \\frac{d \\hat{l}}{d \\theta}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1fdf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First generate a bit more data (n=5000)\n",
    "# your code here ...\n",
    "\n",
    "# we set the size of the batch\n",
    "batch_size = 20\n",
    "\n",
    "# The batching and streaming of data is done for you below\n",
    "\n",
    "# we compute the number of batches to iterate over\n",
    "num_complete_batches, leftover = divmod(num_data, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "print('num_batches: ',  num_batches)\n",
    "\n",
    "# we construct a data stream to easily get access to the batches consecutively\n",
    "def data_stream():\n",
    "    while True:\n",
    "        perm = np.random.permutation(num_data)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield xs[batch_idx], ys[batch_idx]\n",
    "batches = data_stream()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725e18f",
   "metadata": {},
   "source": [
    "We can now train the model on the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8d309",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "params = init(rng)\n",
    "weights = np.zeros((num_batches,))\n",
    "biases = np.zeros((num_batches,))\n",
    "losses = np.zeros((num_batches,))\n",
    "\n",
    "# now code the iterations (for loop) to run the updates on the batches\n",
    "# you can get the next batch via the call 'next(batches)' \n",
    "# store the loss and parameters for later plotting\n",
    "\n",
    "# your code here ... \n",
    "\n",
    "# plot the results (evolution of parameters and loss)\n",
    "\n",
    "# your code here ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96b615",
   "metadata": {},
   "source": [
    "# BONUS Question\n",
    "What does the loss look like? (good exercice to use the vectorization function vmap of jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the loss on a 2d grid of parameters and plot the loss landscape.\n",
    "\n",
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c0fb3",
   "metadata": {},
   "source": [
    "Can you plot the parameter trajectory during optimization on top of the loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
