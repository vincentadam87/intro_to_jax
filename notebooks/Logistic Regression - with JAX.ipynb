{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d8bdea",
   "metadata": {},
   "source": [
    "# Logistic regression with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff822cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7580595",
   "metadata": {},
   "source": [
    "# Learning as gradient-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3bf21",
   "metadata": {},
   "source": [
    "In a typical model based analysis, you have a model with parameters $\\theta$.\n",
    "\n",
    "A datasets consisting in conditions $X$ and observed behavior $Y$.\n",
    "\n",
    "The model provided a `1oss` $l(X, Y, \\theta)$ which quantifies the mismatch between the model predictions and the data. \n",
    "\n",
    "To simplify the notation, I'll drop the dependence on the data, writing $l(\\theta) = l(X, Y, \\theta)$\n",
    "\n",
    "\n",
    "Fitting a model can be cast as finding the parameter $\\hat{\\theta}$ that minimize the loss, i.e. $\\hat{\\theta} = \\arg\\min_{\\theta} l(X, Y, \\theta)$$ \n",
    "\n",
    "Gradient based optimization is set of procedure using the gradient of the loss to guide the search of the optimal parameter $\\hat{\\theta}$.\n",
    "\n",
    "The simplest scheme construct a sequence of parameters $\\theta_1, \\theta_2, ..., \\theta_T$ by 'following the gradient'\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\eta \\frac{d l}{d \\theta}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a347ab",
   "metadata": {},
   "source": [
    "##  Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bbb47",
   "metadata": {},
   "source": [
    "logistic regression \n",
    "\\begin{align}\n",
    "\\phi(x) &= wx + b\\\\\n",
    "y &\\sim Bernoulli( \\sigma( \\phi(x) ))\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1+e^{-x}}$ is the sigmoid function\n",
    "\n",
    "You have \n",
    "* data: $\\{X, Y\\}$ \n",
    "* a linear predictor : $\\rho(x) = w x + b$ (parameters are weights $w$ and bias $b$)\n",
    "* leading to predicted classes $p(y=1|x) = \\sigma(\\rho(x))$\n",
    "* a loss (log likelihood): $ l(w) = \\sum_n y_n \\,\\log\\,p(x) + (1-y_n)\\log(1 - p(x))$\n",
    "\n",
    "We could compute the gradient manually, but with jax we don't need to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be a tiny bit more sophisticated, we'll use a class to hold the parameters\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Params(NamedTuple):\n",
    "    \"\"\" \n",
    "    Class that acts as a container for the parameters.     \n",
    "    you can use it like param = Params(weights, bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    weight: jnp.ndarray\n",
    "    bias: jnp.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an initialization class for the weights\n",
    "\n",
    "def init(rng) -> Params:\n",
    "    \"\"\"Returns the initial model params.\n",
    "    :param rng: random number generator\n",
    "    :return: an instance of the 'Params' class\n",
    "    \"\"\"\n",
    "    weights_key, bias_key = jax.random.split(rng)\n",
    "    weight = jax.random.normal(weights_key, ()) * 3\n",
    "    bias = jax.random.normal(bias_key, ()) * 3\n",
    "    return Params(weight, bias)\n",
    "\n",
    "\n",
    "# we need the sigmoid for the pointwise activations\n",
    "def sigmoid(x: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    The sigmoid function x->1/(x + e^{-x})\n",
    "    \"\"\"\n",
    "    # your code here!\n",
    "    # <SOLUTION\n",
    "    return 1./(1.+jnp.exp(-x))\n",
    "    # SOLUTION>\n",
    "    \n",
    "\n",
    "\n",
    "# we compute our prediction of the label being one\n",
    "def predictions(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    The prediction p(y=1|x).\n",
    "    :param params: an instance of the class Params\n",
    "    :param x: the scalar input\n",
    "    :param y: the binary observation\n",
    "    :return: a probabilty p, 0<p<1 \n",
    "    \"\"\"\n",
    "    # your code here!\n",
    "    # <SOLUTION\n",
    "    eps = 1.e-3\n",
    "    return eps + (1.-2 * eps) * sigmoid(params.weight * x + params.bias)\n",
    "    # SOLUTION>\n",
    "    \n",
    "    \n",
    "# again we define the loss (as defined above)\n",
    "def loss(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the least squares error of the model's predictions on x against y.\n",
    "    :param params: an instance of the class Params\n",
    "    :param x: the scalar input\n",
    "    :param y: the binary observation\n",
    "    :return: the scalar loss \n",
    "    \"\"\"\n",
    "    # your code here!\n",
    "    # <SOLUTION\n",
    "    pred = predictions(params, x, y)\n",
    "    return jnp.mean(y * jnp.log(pred) + (1.-y) * jnp.log(1.-pred))\n",
    "    # SOLUTION>\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "# there is a convenient way to propagate the gradient through to the parameters\n",
    "# even for the more complex structure we now have.\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> Params:\n",
    "    \"\"\"\n",
    "    Performs one SGD update step on params using the given data.\n",
    "    \n",
    "    θ = θ + η grad l(θ)  (with η the learning rate) \n",
    "    \n",
    "    :param params: an instance of the class Params\n",
    "    :param x: the scalar input\n",
    "    :param y: the binary observation\n",
    "    :return: an instance of the Params class containing the updated parameters \n",
    "    \"\"\"\n",
    "    grad = jax.grad(loss)(params, x, y)\n",
    "    new_params = jax.tree_map(\n",
    "      lambda param, g: param + g * LEARNING_RATE, params, grad)\n",
    "\n",
    "    return new_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95a997",
   "metadata": {},
   "source": [
    "Let's generate some data!\n",
    "\n",
    "\\begin{align}\n",
    "x &\\sim {\\cal N}(0,1)\\quad \\text{this means x is Gaussian with mean 0 and variance 1}\\\\\n",
    "\\phi(x) &= wx + b\\\\\n",
    "y &\\sim Bernoulli( \\sigma( \\phi(x) ))\\\\\n",
    "\\end{align}\n",
    "\n",
    "for some chosen values for $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4673c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to prepare jax to sample random variable\n",
    "rng = jax.random.PRNGKey(42)\n",
    "x_rng, noise_rng = jax.random.split(rng)\n",
    "\n",
    "# Generate true data from p = w*x + b, y ~ Bernoulli(p)\n",
    "\n",
    "# we choose some ground true parameters\n",
    "true_w, true_b = 2, -1\n",
    "true_params = Params(true_w, true_b)\n",
    "\n",
    "\n",
    "# we generate 500 data points\n",
    "num_data = 500 \n",
    "# your code here\n",
    "# hint: for x,  you can use `jax.random.normal`\n",
    "# hint: for y,  you can use `jax.random.bernoulli`\n",
    "# <SOLUTION\n",
    "xs = jax.random.normal(x_rng, (num_data, 1))\n",
    "ps = sigmoid(xs * true_w + true_b)\n",
    "ys = jax.random.bernoulli(noise_rng, p=ps)\n",
    "# SOLUTION>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1d598",
   "metadata": {},
   "source": [
    "Let's plot the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802adb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot y as a function of x\n",
    "# plot the predictions for the true parameters p(x) = \\sigma( \\phi(x) )\n",
    "\n",
    "# <SOLUTION\n",
    "plt.plot(xs, ys, '.')\n",
    "plt.plot(xs, predictions(true_params, xs, ys), '*')\n",
    "plt.show()\n",
    "# SOLUTION>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c49f03",
   "metadata": {},
   "source": [
    "Now let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the update function for 200 iterations\n",
    "# and store the intermediate values of the parameters and loss\n",
    "\n",
    "# initializing some arrays for storage\n",
    "num_iterations = 200\n",
    "weights = np.zeros((num_iterations,))\n",
    "biases = np.zeros((num_iterations,))\n",
    "losses = np.zeros((num_iterations,))\n",
    "\n",
    "\n",
    "params = init(rng)\n",
    "for it in range(200):\n",
    "    # <SOLUTION\n",
    "    # run the update\n",
    "    params = update(params, xs, ys)\n",
    "    # store the current params and loss\n",
    "    weights[it] = params.weight\n",
    "    biases[it] = params.bias\n",
    "    losses[it] = loss(params, xs, ys)\n",
    "    # SOLUTION>\n",
    "\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019f537",
   "metadata": {},
   "source": [
    "Let's plot the learning curve and evolution of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75242c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOLUTION\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6,4), sharex=True)\n",
    "axes[0].plot(weights)\n",
    "axes[0].plot(biases)\n",
    "axes[0].set_xlabel('iteration')\n",
    "axes[0].set_ylabel('parameter $w$ & $b$')\n",
    "axes[1].plot(losses)\n",
    "axes[1].set_xlabel('iteration')\n",
    "axes[1].set_ylabel('loss $l(w)$')\n",
    "plt.show()\n",
    "# SOLUTION>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c46f9",
   "metadata": {},
   "source": [
    "## What if there is really a lot of data?\n",
    "\n",
    "\n",
    "If there is a lot of data (think billions), there is no way to evaluate the loss\n",
    "on the whole dataset as we have done so far.\n",
    "\n",
    "We can evaluate the loss on a randomly sampled subset of the data, \n",
    "which leads to an unbiased estimator of the total loss.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{l}(w) &=  l(x_{batch}, y_{batch}, \\theta)\\\\\n",
    "\\mathbb{E}[\\hat{l}(w)] &= l(w)\n",
    "\\end{align}\n",
    "\n",
    "If we replace the gradient of the loss in gradient descent by the gradient of \n",
    "an estimator of the loss we get stochastica gradient descent (SGD)\n",
    "\n",
    "\n",
    "\n",
    "This corresponds to iterate gradient descent steps on batch approximation of the loss.\n",
    "\\begin{align}\n",
    "\\theta_{t+1} &= \\theta_{t} - \\eta \\frac{d \\hat{l}}{d \\theta}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1fdf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we generate a bit more data\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "num_data = 5000\n",
    "xs = jax.random.normal(x_rng, (num_data, 1))\n",
    "ps = sigmoid(xs * true_w + true_b)\n",
    "ys = jax.random.bernoulli(noise_rng, p=ps)\n",
    "\n",
    "# we set the size of the batch\n",
    "batch_size = 20\n",
    "\n",
    "# and compute the number of batches\n",
    "num_complete_batches, leftover = divmod(num_data, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "print('num_batches: ',  num_batches)\n",
    "\n",
    "# we construct a data stream to easily get access to the batches consecutively\n",
    "def data_stream():\n",
    "    while True:\n",
    "        perm = np.random.permutation(num_data)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield xs[batch_idx], ys[batch_idx]\n",
    "batches = data_stream()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725e18f",
   "metadata": {},
   "source": [
    "We can now train the model on the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8d309",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "params = init(rng)\n",
    "weights = np.zeros((num_batches,))\n",
    "biases = np.zeros((num_batches,))\n",
    "losses = np.zeros((num_batches,))\n",
    "\n",
    "# now code the iterations (for loop) to run the updates on the batches\n",
    "# you can get the next batch via the call 'next(batches)' \n",
    "# store the loss and parameters for later plotting\n",
    "\n",
    "# <SOLUTION\n",
    "for j in range(num_batches):\n",
    "    x_batch, y_batch = next(batches)\n",
    "    params = update(params, x_batch, y_batch)\n",
    "    losses[j] = loss(params, x_batch, y_batch)\n",
    "    biases[j] = params.bias\n",
    "    weights[j] = params.weight\n",
    "# SOLUTION>\n",
    "    \n",
    "# plot the results (evolution of parameters and loss)\n",
    "# <SOLUTION\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6,4), sharex=True)\n",
    "axes[0].plot(weights)\n",
    "axes[0].plot(biases)\n",
    "axes[0].set_xlabel('iteration')\n",
    "axes[0].set_ylabel('parameter $w$ & $b$')\n",
    "axes[1].plot(losses)\n",
    "axes[1].set_xlabel('iteration')\n",
    "axes[1].set_ylabel('loss $l(w)$')\n",
    "plt.show()\n",
    "# SOLUTION>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96b615",
   "metadata": {},
   "source": [
    "# BONUS Question\n",
    "What does the loss look like? (good exercice to use the vectorization function vmap of jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOLUTION\n",
    "\n",
    "# plotting the loss\n",
    "from jax import vmap\n",
    "\n",
    "# we have 2 parameters so we need to make a grid \n",
    "n_grid = 100\n",
    "weight_grid = np.linspace(-5,10,n_grid).reshape(1, -1)\n",
    "bias_grid = np.linspace(-10,10,n_grid).reshape(1, -1)\n",
    "w, b = np.meshgrid(weight_grid, bias_grid)\n",
    "w_flat = w.reshape(-1,1)\n",
    "b_flat = b.reshape(-1,1)\n",
    "\n",
    "\n",
    "# conveniently, we can use the vectorization function of jax to evaluate the loss on the whole grid at once\n",
    "loss_tmp = lambda p: loss(p, xs, ys)\n",
    "loss_flat = vmap(loss_tmp)(Params(w_flat, b_flat))\n",
    "loss_grid = loss_flat.reshape((n_grid, n_grid))\n",
    "\n",
    "\n",
    "# let's plot the resulting loss\n",
    "plt.contourf(\n",
    "    loss_grid, \n",
    "    extent=[weight_grid.min(), weight_grid.max(), bias_grid.min(), bias_grid.max()],\n",
    "    origin='lower', levels=20\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.plot(true_w, true_b, 'x')\n",
    "\n",
    "# SOLUTION>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c0fb3",
   "metadata": {},
   "source": [
    "Can you plot the parameter trajectory during optimization on top of the loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <SOLUTION\n",
    "\n",
    "# Running the update\n",
    "params = init(rng)\n",
    "for _ in range(200):\n",
    "    params = update(params, xs, ys)\n",
    "    plt.plot(params.weight, params.bias, 'k.')\n",
    "print('done!')\n",
    "    \n",
    "    \n",
    "# Plotting results\n",
    "plt.contourf(\n",
    "    loss_grid, \n",
    "    extent=[weight_grid.min(), weight_grid.max(), bias_grid.min(), bias_grid.max()],\n",
    "    origin='lower', levels=20\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.plot(true_w, true_b, 'x')\n",
    "plt.show()\n",
    "\n",
    "# SOLUTION>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
