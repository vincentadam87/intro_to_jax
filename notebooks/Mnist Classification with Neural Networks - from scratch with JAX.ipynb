{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842c855c",
   "metadata": {},
   "source": [
    "# Mnist Classification with Neural Networks - from scratch with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60823b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !git clone https://github.com/vincentadam87/intro_to_jax.git\n",
    "    import sys  \n",
    "    sys.path.insert(0,'/content/intro_to_jax/notebooks')\n",
    "\n",
    "import datasets  # check you have the datasets.py module next to this notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "from jax import jit, grad\n",
    "from jax.scipy.special import logsumexp\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ddadec",
   "metadata": {},
   "source": [
    "## What are deep neural networks?\n",
    "\n",
    "\n",
    "Tensorflow is popular because it enables to fit popular models: deep neural networks.\n",
    "\n",
    "A deep neural network is a model made of combination of simple artificial neurons, which are an abstraction of what a neuron is.\n",
    "\n",
    "### Artificial neuron \n",
    "A neuron receives many inputs and spits an output in a non-linear fashion.\n",
    "$$ y = \\phi( W {\\bf x} + b )$$\n",
    "where $\\phi$ is a non-linearity such as the sigmoid function.\n",
    "\n",
    "__[Remark]__ bold characters represent vectors or matrices\n",
    "\n",
    "### Hidden layer\n",
    "\n",
    "A neural network typically combines these in a sequence, so the output of some neurons serves as input to other neurons. Here is an example of two neurons where the output $h_1$ of neuron $1$ is the input of neuron $2$.\n",
    "Letter $h$ is used to refer to a `hidden` layer\n",
    "$$ h_1 = \\phi(W_1 {\\bf x} + b_1) $$\n",
    "$$ y = \\phi(W_2 h_1 + b_2) $$\n",
    "\n",
    "### Wide networks\n",
    "\n",
    "In practice, a layer may be `wide` and consist of many neurons sharing the same input\n",
    "$$ {\\bf h}_1 = \\phi(W_1 {\\bf x} + b_1) $$\n",
    "\n",
    "\n",
    "### Deep networks \n",
    "\n",
    "And a neuron network may be `deep` and consist of many layers\n",
    "$$ {\\bf h}_1 = \\phi(W_1 {\\bf x} + b_1) $$\n",
    "$$ \\vdots $$\n",
    "$$ {\\bf h}_K = \\phi(W_K {\\bf h}_{K-1} + b_K) $$\n",
    "$$ y = \\phi(W_{K+1} {\\bf h}_K + b_K) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a42a870",
   "metadata": {},
   "source": [
    "# Task : Image Classification\n",
    "\n",
    "Let's look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the data loader provided splits both the images and labels into\n",
    "# a training and test set\n",
    "train_images, train_labels, test_images, test_labels = datasets.mnist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e034ac",
   "metadata": {},
   "source": [
    "__Question__\n",
    "\n",
    "* Can you have a look at the data ? What are the shapes of X and Y?\n",
    "\n",
    "* Can you plot some images and associated label?\n",
    "\n",
    "* Do you think logistic regression would work for these complex inputs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print a few images and check the size/shape of the data\n",
    "# <SOLUTION\n",
    "print(train_images.shape)\n",
    "fig, axarr = plt.subplots(4,4, figsize=(5,5))\n",
    "for i, ax in enumerate(axarr.flatten()):\n",
    "    ax.imshow(train_images[i].reshape(28,28))\n",
    "    ax.set_title(\"label = %i\" %np.argmax(train_labels[i]))\n",
    "fig.tight_layout()\n",
    "# SOLUTION>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f592887",
   "metadata": {},
   "source": [
    "We want to construct a general model of the form\n",
    "\n",
    "\\begin{align} \n",
    "{\\bf h}_1 &= \\phi(W_1 {\\bf x} + b_1)\\quad \\text{  x is a 28 by 28 image}\\\\\n",
    "&\\vdots \\\\\n",
    "{\\bf h}_{K-1} &= \\phi(W_{K-1} {\\bf h}_{K-2} + b_{K-1}) \\quad\\text{  the nextwork has a total of K layers} \\\\\n",
    "p &= \\phi(W_{K} {\\bf h}_{K-1} + b_{K}) \\quad \\text{  p is a vector of class probability of size 10}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward model of the neural network from image x to class label probability\n",
    "\n",
    "\n",
    "def predict(params, inputs):\n",
    "    \"\"\"\n",
    "    Computing the a deep network\n",
    "    \n",
    "    :param params: the model parameters\n",
    "    :param inputs: the input images\n",
    "    \n",
    "    :return: a vector of the predicted log probabilities of each class, log p(y=i|x) \n",
    "    \"\"\"\n",
    "    \n",
    "    # sequential call of each layer\n",
    "    # the output of a layer is the input of the next layer\n",
    "    activations = inputs\n",
    "    for w, b in params[:-1]:\n",
    "        # outputs = ... # your code here\n",
    "        outputs = jnp.dot(activations, w) + b\n",
    "        # activations = ... your code here\n",
    "        # <SOLUTION\n",
    "        activations = jnp.tanh(outputs)\n",
    "        # SOLUTION>\n",
    "        \n",
    "    # the final layer is treated separately\n",
    "    final_w, final_b = params[-1]\n",
    "    # outputs are the log probabilities of each class (logits = log p(y|x))\n",
    "    logits = ... # your code here\n",
    "    # <SOLUTION\n",
    "    pre_logits = jnp.dot(activations, final_w) + final_b\n",
    "    logits = pre_logits - logsumexp(pre_logits, axis=1, keepdims=True)\n",
    "    # SOLUTION>\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random inital weights and biases for a neural network\n",
    "\n",
    "def init_random_params(layer_sizes, scale=0.1, rng=npr.RandomState(0)):\n",
    "    \"\"\"\n",
    "    :param scale: magnitude of the random noise initialisation\n",
    "    :param layer_sizes: list of the input/output size of the layers of the network\n",
    "    :param rng: Random Number Generator, an object to sample random variables\n",
    "    \n",
    "    :return: a list of lists of parameters for each layer (weight and bias)\n",
    "    \"\"\"\n",
    "\n",
    "    return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9f9d8",
   "metadata": {},
   "source": [
    "### The classification loss\n",
    "\n",
    "We are here doing multiclass classification \n",
    "\n",
    "the loss is \n",
    "$$ l(w) = \\sum_n \\log\\,p(y=y_n|x=x_n) $$\n",
    "\n",
    "We are working with a one-hot encoding of the labels, (class = 2 $\\to$ `y=[0,0,1,0,0,0,0,0,0]`)\n",
    "\n",
    "This is simpler to implement as follows\n",
    "\n",
    "$$ l(w) = \\sum_n \\sum_c \\log\\,p(y=c|x=x_n) * \\delta(y_n = c)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss of the classifier: it is the log likelihood of the observations\n",
    "\n",
    "def loss(params, batch):\n",
    "    \"\"\"\n",
    "    :param params: the parameters of the neural network\n",
    "    :param batch: a list of the imput images and associated label\n",
    "    \n",
    "    :return: a scalar number, the loss to be minimized\n",
    "    \"\"\"\n",
    "    inputs, targets = batch\n",
    "    # your code here\n",
    "    # <SOLUTION\n",
    "    preds = predict(params, inputs)\n",
    "    return -jnp.mean(jnp.sum(preds * targets, axis=1))\n",
    "    # SOLUTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271587f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the update method to run gradient descent\n",
    "\n",
    "@jit\n",
    "def update(params, batch):\n",
    "    grads = grad(loss)(params, batch)\n",
    "    return [(w - learning_rate * dw, b - learning_rate * db)\n",
    "            for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do with a metric a more interpretable metric to evaluate how well we do\n",
    "\n",
    "def accuracy(params, batch):\n",
    "    \"\"\"\n",
    "    The accuracy is the fraction of correct predictions.\n",
    "    \n",
    "    :param params: the parameters of the neural network\n",
    "    :param batch: the imput images \n",
    "    \"\"\"\n",
    "    # <SOLUTION\n",
    "    inputs, targets = batch\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "    # SOLUTION>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a171ce",
   "metadata": {},
   "source": [
    "# Now the main learning routine\n",
    "\n",
    "## Let's start with Logistic regression\n",
    "\n",
    "Logistic regression corresponds to a 1 layer neural network\n",
    "\n",
    "\\begin{align} \n",
    "p &= \\phi(W_{1} {\\bf x} + b_{1}) \\quad \\text{  p is a vector of class probability of size 10}\n",
    "\\end{align}\n",
    "\n",
    "In this code this corresponds to `layer_sizes = [784, 10]`\n",
    "\n",
    "Later we'll add layers easily: `layer_sizes = [784, 1024, ..., 1024 , 10]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727998f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the network structure\n",
    "batch_size = 128\n",
    "layer_sizes = [784, 10]  # this is logistic regression!\n",
    "# layer_sizes = [784, 1024, 1024, 10] # try that later for deeper networks\n",
    "\n",
    "# setting up the training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30 # number of passes throught the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to do stochastic gradient descent and evaluate the loss on batches of data\n",
    "\n",
    "num_train = train_images.shape[0]\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data stream to easily get the batches\n",
    "def data_stream():\n",
    "    rng = npr.RandomState(0)\n",
    "    while True:\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield train_images[batch_idx], train_labels[batch_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71758032",
   "metadata": {},
   "source": [
    "Now the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preparing the data stream\n",
    "batches = data_stream()\n",
    "\n",
    "\n",
    "# initializing the parameters\n",
    "params = init_random_params(layer_sizes)\n",
    "\n",
    "# iterate over epochs (an epoch is one full sweep through the entire dataset)\n",
    "# for each epoch, iterate over all batches\n",
    "# print the accuracy after each epoch\n",
    "\n",
    "\n",
    "# <SOLUTION\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    # iterating over all batches in the dataset\n",
    "    for _ in range(num_batches):\n",
    "        params = update(params, next(batches))\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    train_acc = accuracy(params, (train_images, train_labels))\n",
    "    test_acc = accuracy(params, (test_images, test_labels))\n",
    "    print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
    "    print(f\"Training set accuracy {train_acc}\")\n",
    "    print(f\"Test set accuracy {test_acc}\")\n",
    "# SOLUTION>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157003d0",
   "metadata": {},
   "source": [
    "# BONUS: Interpreting the learned features\n",
    "\n",
    "Let's have a look at the weights of the network (for logistic regression)\n",
    "Since the weights for each class is the size of an image, it can be plotted and interpreted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68442c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature image (the w) for each class \n",
    "\n",
    "# <SOLUTION\n",
    "# extract the weights from the params\n",
    "weights = params[0][0].reshape(28,28,10)\n",
    "biases = params[0][1]\n",
    "\n",
    "# plot the weights\n",
    "fig, axarr = plt.subplots(2,5, figsize=(8, 5))\n",
    "for i, ax in enumerate(axarr.flatten()):\n",
    "    ax.imshow(weights[..., i])\n",
    "    ax.set_title(\"feature %i\" %i)\n",
    "fig.tight_layout()\n",
    "\n",
    "# SOLUTION>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0725d9",
   "metadata": {},
   "source": [
    "## BONUS:  Deeper networks\n",
    "\n",
    "Now let's go deeper!\n",
    "\n",
    "* can you re-rerun this code by changing the neural network architecture (adding layers)?\n",
    "\n",
    "just change `layer_sizes` in the code you have (maybe note down the previous final accuracy before!)\n",
    "\n",
    "\n",
    "* What happens to the accuracy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
