{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842c855c",
   "metadata": {},
   "source": [
    "# Mnist Classification with Neural Networks - from scratch with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60823b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !git clone https://github.com/vincentadam87/intro_to_jax.git\n",
    "    import sys  \n",
    "    sys.path.insert(0,'/content/intro_to_jax/notebooks')\n",
    "\n",
    "import datasets  # check you have the datasets.py module next to this notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "from jax import jit, grad\n",
    "from jax.scipy.special import logsumexp\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ddadec",
   "metadata": {},
   "source": [
    "## What are deep neural networks?\n",
    "\n",
    "\n",
    "Tensorflow is popular because it enables to fit popular models: deep neural networks.\n",
    "\n",
    "A deep neural network is a model made of combination of simple artificial neurons, which are an abstraction of what a neuron is.\n",
    "\n",
    "### Artificial neuron \n",
    "A neuron receives many inputs and spits an output in a non-linear fashion.\n",
    "$$ y = \\phi( W {\\bf x} + b )$$\n",
    "where $\\phi$ is a non-linearity such as the sigmoid function.\n",
    "\n",
    "__[Remark]__ bold characters represent vectors or matrices\n",
    "\n",
    "### Hidden layer\n",
    "\n",
    "A neural network typically combines these in a sequence, so the output of some neurons serves as input to other neurons. Here is an example of two neurons where the output $h_1$ of neuron $1$ is the input of neuron $2$.\n",
    "Letter $h$ is used to refer to a `hidden` layer\n",
    "$$ h_1 = \\phi(W_1 {\\bf x} + b_1) $$\n",
    "$$ y = \\phi(W_2 h_1 + b_2) $$\n",
    "\n",
    "### Wide networks\n",
    "\n",
    "In practice, a layer may be `wide` and consist of many neurons sharing the same input\n",
    "$$ {\\bf h}_1 = \\phi(W_1 {\\bf x} + b_1) $$\n",
    "\n",
    "\n",
    "### Deep networks \n",
    "\n",
    "And a neuron network may be `deep` and consist of many layers\n",
    "$$ {\\bf h}_1 = \\phi(W_1 {\\bf x} + b_1) $$\n",
    "$$ \\vdots $$\n",
    "$$ {\\bf h}_K = \\phi(W_K {\\bf h}_{K-1} + b_K) $$\n",
    "$$ y = \\phi(W_{K+1} {\\bf h}_K + b_K) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a42a870",
   "metadata": {},
   "source": [
    "# Task : Image Classification\n",
    "\n",
    "Let's look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the data loader provided splits both the images and labels into\n",
    "# a training and test set\n",
    "train_images, train_labels, test_images, test_labels = datasets.mnist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e034ac",
   "metadata": {},
   "source": [
    "__Question__\n",
    "\n",
    "* Can you have a look at the data ? What are the shapes of X and Y?\n",
    "\n",
    "* Can you plot some images and associated label?\n",
    "\n",
    "* Do you think logistic regression would work for these complex inputs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print a few images and check the size/shape of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97388c",
   "metadata": {},
   "source": [
    "## Forward model \n",
    "\n",
    "We want to construct a general model of the form\n",
    "\n",
    "\\begin{align} \n",
    "{\\bf h}_1 &= \\phi(W_1 {\\bf x} + b_1)\\quad \\text{  x is a 28 by 28 image}\\\\\n",
    "&\\vdots \\\\\n",
    "{\\bf h}_{K-1} &= \\phi(W_{K-1} {\\bf h}_{K-2} + b_{K-1}) \\quad\\text{  the nextwork has a total of K layers} \\\\\n",
    "p &= \\psi(W_{K} {\\bf h}_{K-1} + b_{K}) \\quad \\text{  p is a vector of class probability of size 10}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976eb320",
   "metadata": {},
   "source": [
    "## Parameterization of the network through layer shapes\n",
    "\n",
    " We work with parameters that lists of lists\n",
    " `params = [params_1, ..., params_K]`\n",
    " \n",
    "with `params_i = [weight_i, bias_i]` with shapes \n",
    "* `weight_i: [output_dim_i, input_dim_i]` \n",
    "* `bias_i: [output_dim_i]`\n",
    "\n",
    "Because the output of a layer is the input of the subsequent layer, we have\n",
    "`output_dim_{i} = input_dim_{i+1}`\n",
    "\n",
    "All we need need is to specify a list of layer_sizes:\n",
    "`layer_sizes = [input_dim_1, input_dim_2, ..., output_dim_K]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random inital weights and biases for a neural network: this is done for you\n",
    "\n",
    "def init_random_params(layer_sizes, scale=0.1, rng=npr.RandomState(0)):\n",
    "    \"\"\"\n",
    "    Constructs random parameters for the network specified by the layer_sizes\n",
    "    \n",
    "    For layer_sizes = [size_1, ..., size_K], we have\n",
    "    weights_1 with shape (size_2, size_1), bias_1 with shape (size_2,)\n",
    "    ...\n",
    "    weights_{K-1} with shape (size_K, size_{K-1}), bias_{K-1} with shape (size_{K-1})\n",
    "    \n",
    "    :param scale: magnitude of the random noise initialisation\n",
    "    :param layer_sizes: list of the input/output size of the layers of the network\n",
    "    :param rng: Random Number Generator, an object to sample random variables\n",
    "    \n",
    "    :return: a list of lists of parameters for each layer (weight and bias)\n",
    "    \"\"\"\n",
    "\n",
    "    return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e44aa76",
   "metadata": {},
   "source": [
    "## Softmax link function\n",
    "\n",
    "The forward model of the neural network from image $x$ to class label probability $p(y=c|x)$ as follows\n",
    "\n",
    "The last layer of the network computes the  $l = W_{K} {\\bf h}_{K-1} + b_{K}$, which is a vector of size 10.\n",
    "\n",
    "This vector is used to create probabilities for the class\n",
    "$$ p(y=c|l) = \\frac{\\exp (l_c)}{\\sum_{c'} \\exp (l_{c'})} $$\n",
    "\n",
    "The log-probabities are thus:\n",
    "$$ \\log\\,p(y=c|l) = l_c - \\log \\big({\\sum_{c'} \\exp (l_{c'})} \\big)$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the forward model from inputs to class assignment probabilities\n",
    "\n",
    "def predict(params, inputs):\n",
    "    \"\"\"\n",
    "    Computing the a deep network\n",
    "    \n",
    "    :param params: the model parameters\n",
    "    :param inputs: the input images, with shape (N, 784)\n",
    "    \n",
    "    :return: a vector of the predicted log probabilities of each class, log p(y=i|x), with shape (N, 10)\n",
    "    \"\"\"\n",
    "    \n",
    "    # sequential call of each layer\n",
    "    # the output of a layer is the input of the next layer\n",
    "    activations = inputs\n",
    "    for w, b in params[:-1]:\n",
    "        # outputs = ... # your code here\n",
    "        # activations = ... your code here (you can use the jnp.tanh non linearity)\n",
    "        \n",
    "    # the final layer is treated separately\n",
    "    final_w, final_b = params[-1]\n",
    "    # outputs are the log probabilities of each class (logits = log p(y|x))\n",
    "    # your code here ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9f9d8",
   "metadata": {},
   "source": [
    "### The classification loss\n",
    "\n",
    "We are here doing multiclass classification \n",
    "\n",
    "the loss is the negative log-likelihood\n",
    "$$ l(w) = - \\sum_n \\log\\,p(y=y_n|x=x_n) $$\n",
    "\n",
    "We are working with a one-hot encoding of the labels, (class = 2 $\\to$ `y=[0,0,1,0,0,0,0,0,0]`)\n",
    "\n",
    "This is simpler to implement as follows\n",
    "\n",
    "$$ l(w) = - \\sum_n \\sum_c \\log\\,p(y=c|x=x_n) * \\delta(y_n = c)$$\n",
    "\n",
    "where $\\delta$ is the dirac function ($=1$ when the argument is true, $0$ otherwise)\n",
    "\n",
    "so $\\delta(y_n = c)$ is the one-hot encoding of observation $y_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss of the classifier: it is the log likelihood of the observations\n",
    "\n",
    "def loss(params, batch):\n",
    "    \"\"\"\n",
    "    :param params: the parameters of the neural network\n",
    "    :param batch: a list of the imput images and associated label with shape [(N, 784), (N, 10)]\n",
    "    :return: a scalar number, the loss to be minimized, with shape (,)\n",
    "    \"\"\"\n",
    "    # your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271587f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the update method to run gradient descent\n",
    "\n",
    "@jit\n",
    "def update(params, batch, learning_rate):\n",
    "    \"\"\"\n",
    "    :param params: the parameters of the neural network\n",
    "    :param batch: a list of the imput images and associated label with shape [(N, 784), (N, 10)]\n",
    "    :param learning_rate: the scalar learning rate\n",
    "    :return: the updated params after a step of gradient descent\n",
    "    \"\"\"\n",
    "    # call the gradient (which will have the list structure of the params)\n",
    "    # and loop over layers to apply a step of gradient descent to each\n",
    "    # your code here ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do with a metric a more interpretable metric to evaluate how well we do\n",
    "\n",
    "def accuracy(params, batch):\n",
    "    \"\"\"\n",
    "    The accuracy is the fraction of correct predictions.\n",
    "    :param params: the parameters of the neural network\n",
    "    :param batch: a list of the imput images and associated label with shape [(N, 784), (N, 10)]\n",
    "    :return: the accuracy, a scalar\n",
    "    \"\"\"\n",
    "    # your code here ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a171ce",
   "metadata": {},
   "source": [
    "# Now the main learning routine\n",
    "\n",
    "## Let's start with Logistic regression\n",
    "\n",
    "Logistic regression corresponds to a 1 layer neural network\n",
    "\n",
    "\\begin{align} \n",
    "p &= \\phi(W_{1} {\\bf x} + b_{1}) \\quad \\text{  p is a vector of class probability of size 10}\n",
    "\\end{align}\n",
    "\n",
    "In this code this corresponds to `layer_sizes = [784, 10]`\n",
    "\n",
    "Later we'll add layers easily: `layer_sizes = [784, 1024, ..., 1024 , 10]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727998f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the network structure\n",
    "batch_size = 128\n",
    "layer_sizes = [784, 10]  # this is logistic regression!\n",
    "# layer_sizes = [784, 1024, 1024, 10] # try that later for deeper networks\n",
    "\n",
    "# setting up the training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30 # number of passes throught the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to do stochastic gradient descent and evaluate the loss on batches of data\n",
    "\n",
    "num_train = train_images.shape[0]\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data stream to easily get the batches\n",
    "def data_stream():\n",
    "    rng = npr.RandomState(0)\n",
    "    while True:\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield train_images[batch_idx], train_labels[batch_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71758032",
   "metadata": {},
   "source": [
    "Now the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preparing the data stream\n",
    "batches = data_stream()\n",
    "\n",
    "\n",
    "# initializing the parameters\n",
    "params = init_random_params(layer_sizes)\n",
    "\n",
    "# iterate over epochs (an epoch is one full sweep through the entire dataset)\n",
    "# for each epoch, iterate over all batches\n",
    "# print the accuracy after each epoch\n",
    "# your code here ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157003d0",
   "metadata": {},
   "source": [
    "# BONUS: Interpreting the learned features\n",
    "\n",
    "Let's have a look at the weights of the network (for logistic regression)\n",
    "\n",
    "Since the weights for each class is the size of an image, it can be plotted and interpreted!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68442c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature image (the w you can extract from params) for each class \n",
    "# your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0725d9",
   "metadata": {},
   "source": [
    "## BONUS:  Deeper networks\n",
    "\n",
    "Now let's go deeper!\n",
    "\n",
    "* can you re-rerun this code by changing the neural network architecture (adding layers)?\n",
    "\n",
    "just change `layer_sizes` in the code you have (maybe note down the previous final accuracy before!)\n",
    "\n",
    "\n",
    "* What happens to the accuracy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
