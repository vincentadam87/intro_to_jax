{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc54dcbd",
   "metadata": {},
   "source": [
    "# Linear Regression - with and without JAX\n",
    "\n",
    "### Aims of the notebook\n",
    "* code a model and perform gradient based learning via gradient descent\n",
    "* do it both with numpy and jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff822cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7580595",
   "metadata": {},
   "source": [
    "# Learning as gradient-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3bf21",
   "metadata": {},
   "source": [
    "In a typical model based analysis, you have a model with parameters $\\theta$.\n",
    "\n",
    "A datasets consisting in conditions $X$ and observed behavior $Y$.\n",
    "\n",
    "The model provided a `1oss` $l(X, Y, \\theta)$ which quantifies the mismatch between the model predictions and the data. \n",
    "\n",
    "To simplify the notation, we drop the dependence on the data, writing $l(\\theta) = l(X, Y, \\theta)$\n",
    "\n",
    "\n",
    "Fitting a model can be cast as finding the parameter $\\hat{\\theta}$ that minimize the loss, i.e. $\\hat{\\theta} = \\arg\\min_{\\theta} l(X, Y, \\theta)$$ \n",
    "\n",
    "Gradient based optimization is set of procedure using the gradient of the loss to guide the search of the optimal parameter $\\hat{\\theta}$.\n",
    "\n",
    "The simplest scheme construct a sequence of parameters $\\theta_1, \\theta_2, ..., \\theta_T$ by 'following the gradient'\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\eta \\frac{d l}{d \\theta}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e238ddc3",
   "metadata": {},
   "source": [
    "### Example of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb3c1d",
   "metadata": {},
   "source": [
    "Introductory example: linear regression $y = 2 x + \\epsilon$, with $\\epsilon \\sim {\\cal N}(0, 1)$\n",
    "\n",
    "You have \n",
    "* data: $\\{X, Y\\}$ \n",
    "* a parametric model: $\\rho(x) = w x$ (here $\\theta=w$)\n",
    "* a loss: $ l(w) = \\frac{1}{n} \\sum_n (y_n - w x_n)^2$\n",
    "\n",
    "In this simple example, the gradient of the loss wrt $w$ is available :\n",
    "$$\\frac{d l}{d w} = -\\frac{2}{n}  \\sum_n x_n (y_n - w x_n)$$\n",
    "\n",
    "This gradient tells gives you how the loss $l(w)$ changes as you vary $w$.\n",
    "\n",
    "A typical gradient based optimization scheme 'follows the gradient'\n",
    "$$ w_{t+1} = w_{t} - \\eta \\frac{d l}{d w}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e6e85",
   "metadata": {},
   "source": [
    "# Linear Regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea27311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Linear regression with manual gradients\n",
    "\n",
    "# Here is toy dataset (X and Y) defined by Y = 2 X + noise\n",
    "N = 100\n",
    "X_np = np.random.rand(N, 1) \n",
    "\n",
    "# for linear regression\n",
    "W_true = 2.\n",
    "P_np = W_true * X_np \n",
    "Y_np = W_true * X_np + np.random.randn(N, 1) * .5\n",
    "\n",
    "# plot the data set (Y as a function of X)\n",
    "# your code here ...\n",
    "\n",
    "# plot the optimal prediction (2 X as a function of X) for a grid of X values\n",
    "# your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ad74e",
   "metadata": {},
   "source": [
    "Let's now build the model and the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the model\n",
    "\n",
    "# linear model\n",
    "def prediction(W, X):\n",
    "    \"\"\"\n",
    "    Prediction rho(X,W) = W X\n",
    "    :param W: weights, [batch_shape] + (1,)\n",
    "    :param X: inputs, with shape (N, 1)\n",
    "    :return: the predictor, with shape [batch_shape] + (N,)\n",
    "    \"\"\"\n",
    "    # code here the linear predictor p =  X * W\n",
    "    # your code here ...\n",
    "\n",
    "# loss\n",
    "def loss(W, X, Y):\n",
    "    \"\"\"\n",
    "    the loss for linear regression l(W) = sum_n ||rho(X_n,W) - Y_n||^2\n",
    "    :param W: weights, [batch_shape] + (1,)\n",
    "    :param Y: outputs, with shape (N, 1)\n",
    "    :return: the loss, with shape [batch_shape] + (,)\n",
    "    \"\"\"\n",
    "    # code here the loss function  l = sum_n (X_n*W - Y_n)^2\n",
    "    # your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f481f96",
   "metadata": {},
   "source": [
    "Let's plot the loss as a function of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid of parameter values on the interval [-5, 5]\n",
    "# your code here ...\n",
    "\n",
    "# compute the loss for this grid of parameter values\n",
    "# your code here ...\n",
    "\n",
    "# plot the loss l(w) as a function of w\n",
    "# your code here ...\n",
    "\n",
    "# can you eye ball the parameter value minimizing the loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcd596",
   "metadata": {},
   "source": [
    "__Question__ : Can you eye ball the parameter value minimizing the loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cbbda9",
   "metadata": {},
   "source": [
    "Now let's code up the gradient of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253464de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss gradient (manually coded)\n",
    "def grad_loss(W, X, Y):\n",
    "    \"\"\"\n",
    "    gradient of the loss for linear regression, with respect to W\n",
    "    :param W: weights, with shape [batch_shape] + (1,)\n",
    "    :param X: inputs, with shape (N, 1)\n",
    "    :param Y: outputs, with shape (N, 1)\n",
    "    :return: gradient of the loss, with shape [batch_shape] + (N, 1)\n",
    "    \"\"\"\n",
    "    # code here grad = - 2 sum_n X_n(Y_n - W X_n)\n",
    "\n",
    "# parameter update function\n",
    "def update(W, X, Y, learning_rate):\n",
    "    \"\"\"\n",
    "    A step of gradient descent\n",
    "    :param W: weights, with shape [batch_shape] + (1,)\n",
    "    :param X: inputs, with shape (N, 1)\n",
    "    :param Y: outputs, with shape (N, 1)\n",
    "    :param learning_rate: the scalar learning rate\n",
    "    :return: the updated weights following the gradient descent update rule, with shape [batch_shape] + (1,)\n",
    "    \"\"\"\n",
    "    # your code here ...\n",
    "\n",
    "# gradient descent\n",
    "\n",
    "# we choose a learning rate\n",
    "learning_rate = 1e-1 # the speed at which you follow the gradient\n",
    "# we choose an initial guess\n",
    "W0 = -1. # initial guess \n",
    "\n",
    "\n",
    "# Code here the script to follow the gradient a 100 time\n",
    "# (storing the intermediate parameter and loss values)\n",
    "\n",
    "\n",
    "# plot the parameter trajectory (values as a function of the iteration)\n",
    "# plot the loss as a function of the iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ada550",
   "metadata": {},
   "source": [
    "__Remark__: the choice of the optimization procedure is important\n",
    "    \n",
    "__Question__: What happens if you change the `learning_rate`? Try `0.1`, or `0.0001`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a256ce",
   "metadata": {},
   "source": [
    "## Linear Regression using JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bfec39",
   "metadata": {},
   "source": [
    "JAX has been created to be close to numpy.\n",
    "As a result, the code is going to be very similar.\n",
    "\n",
    "you can almost always replace `np.some_operation` \n",
    "by it's jax equivalent `jnp.some_operation`\n",
    "\n",
    "The main advantage of JAX for our purpose is the fact it can compute the gradient for you\n",
    "via the `jax.grad` operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_jax(W, X, Y):\n",
    "    \"\"\"\n",
    "    the loss for linear regression l(W) = sum_n ||rho(X_n,W) - Y_n||^2\n",
    "    :param W: weights, scalar\n",
    "    :param X: inputs, with shape (N, 1)\n",
    "    :param Y: outputs, with shape (N, 1)\n",
    "    :return: the loss, scalar\n",
    "    \"\"\"\n",
    "    # code here the loss function  l = sum_n (X_n*W - Y_n)^2\n",
    "\n",
    "def grad_loss_jax(W, X, Y):\n",
    "    \"\"\"\n",
    "    gradient of the loss for linear regression, with respect to W\n",
    "    :param W: weights, scalar\n",
    "    :param X: inputs, with shape (N, 1)\n",
    "    :param Y: outputs, with shape (N, 1)\n",
    "    :return: gradient of the loss, scalar\n",
    "    \"\"\"\n",
    "    # no need to do the gradient manually! use jax.grad\n",
    "\n",
    "@jax.jit\n",
    "def update_jax(W, X, Y, learning_rate):\n",
    "    \"\"\"\n",
    "    A step of gradient descent\n",
    "    :param W: weights, scalar\n",
    "    :param X: inputs, with shape (N, 1)\n",
    "    :param Y: outputs, with shape (N, 1)\n",
    "    :param learning_rate: the scalar learning rate\n",
    "    :return: the updated weights following the gradient descent update rule, scalar\n",
    "    \"\"\"    \n",
    "    \n",
    "# Note: all these functions work for scalar weights, to evaluate the loss on a grid of weights,\n",
    "# you may use the 'vmap' function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ece8a1",
   "metadata": {},
   "source": [
    "Now let's right the main training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65693350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code up the main training loop, store the intermediate parameter and loss values to plot their evolution in time\n",
    "\n",
    "\n",
    "# plot the parameter trajectory (values as a function of the iteration)\n",
    "# plot the loss as a function of the iteration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
